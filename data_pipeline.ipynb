{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon API Key (Replace with your own)\n",
    "POLYGON_API_KEY = 'uwQtl3txGt5BLbecq7ZbIu0ZbuitCGjc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amv10802/Documents/Flirting-With-Models/flirtingEnv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([51])) that is different to the input size (torch.Size([50])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add sequence length dimension\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     model \u001b[38;5;241m=\u001b[39m StockTransformer(\n\u001b[1;32m    136\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 118\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, train_labels, epochs, lr)\u001b[0m\n\u001b[1;32m    116\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    117\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(train_data)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 118\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    120\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Flirting-With-Models/flirtingEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Flirting-With-Models/flirtingEnv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Flirting-With-Models/flirtingEnv/lib/python3.11/site-packages/torch/nn/modules/loss.py:699\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Flirting-With-Models/flirtingEnv/lib/python3.11/site-packages/torch/nn/functional.py:3560\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3563\u001b[0m     )\n\u001b[1;32m   3565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3566\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([51])) that is different to the input size (torch.Size([50])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Function to fetch stock data from Polygon\n",
    "def fetch_stock_data(symbol, start_date, end_date):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/range/1/day/{start_date}/{end_date}?apiKey={POLYGON_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'results' not in data:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(data['results'])\n",
    "    df['date'] = pd.to_datetime(df['t'], unit='ms').dt.strftime('%m-%d-%Y')\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.rename(columns={'o': 'open', 'h': 'high', 'l': 'low',\n",
    "              'c': 'close', 'v': 'volume'}, inplace=True)\n",
    "    return df[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "# Function to fetch fundamental data from Polygon\n",
    "\n",
    "\n",
    "def fetch_fundamental_data(symbol):\n",
    "    url = f\"https://api.polygon.io/v2/reference/financials/{symbol}?apiKey={POLYGON_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'results' not in data:\n",
    "        return None\n",
    "\n",
    "    metrics = data['results'][0]\n",
    "    ev_ebitda = metrics['enterpriseValue'] / \\\n",
    "        metrics['earningsBeforeInterestTaxesDepreciationAmortization'] if metrics[\n",
    "            'earningsBeforeInterestTaxesDepreciationAmortization'] else np.nan\n",
    "    pe_ratio = metrics['priceToEarningsRatio']\n",
    "\n",
    "    return {'EV/EBITDA': ev_ebitda, 'P/E': pe_ratio}\n",
    "\n",
    "# Function to compute technical indicators\n",
    "\n",
    "\n",
    "def compute_technical_indicators(df):\n",
    "    df['RSI'] = compute_rsi(df['close'])\n",
    "    df['Stoch'] = compute_stochastic(df['close'], df['low'], df['high'])\n",
    "    df['SMA_50'] = df['close'].rolling(window=50).mean()\n",
    "    df['SMA_200'] = df['close'].rolling(window=200).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# RSI Calculation\n",
    "\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Stochastic Oscillator Calculation\n",
    "\n",
    "\n",
    "def compute_stochastic(close, low, high, period=14):\n",
    "    lowest_low = low.rolling(window=period).min()\n",
    "    highest_high = high.rolling(window=period).max()\n",
    "    return ((close - lowest_low) / (highest_high - lowest_low)) * 100\n",
    "\n",
    "# Fetch and process data\n",
    "\n",
    "\n",
    "def prepare_data(symbol, start_date, end_date):\n",
    "    df = fetch_stock_data(symbol, start_date, end_date)\n",
    "    fundamentals = fetch_fundamental_data(symbol)\n",
    "\n",
    "    if df is not None and fundamentals is not None:\n",
    "        df = compute_technical_indicators(df)\n",
    "        df['EV/EBITDA'] = fundamentals['EV/EBITDA']\n",
    "        df['P/E'] = fundamentals['P/E']\n",
    "        df.to_csv(f'{symbol}.csv',index=True)\n",
    "\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(df.drop(columns=['close']))\n",
    "        labels = (df['close'].shift(-1) > df['close']).astype(int).dropna()\n",
    "        # Align features and labels\n",
    "        features, labels = features[:-1], labels.values\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "    return None, None\n",
    "\n",
    "# Transformer Model for Binomial Classification\n",
    "\n",
    "\n",
    "class StockTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, ff_dim, dropout=0.1):\n",
    "        super(StockTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layers, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 1)  # Logistic regression layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# Training setup\n",
    "\n",
    "\n",
    "def train_model(model, train_data, train_labels, epochs=50, lr=0.001):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_data).squeeze()\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "symbol = \"TSLA\"\n",
    "features, labels = prepare_data(symbol, start_date, end_date)\n",
    "\n",
    "if features is not None and labels is not None:\n",
    "    features = features.unsqueeze(1)  # Add sequence length dimension\n",
    "    model = StockTransformer(\n",
    "        input_dim=features.shape[2], embed_dim=64, num_heads=4, num_layers=2, ff_dim=128)\n",
    "    train_model(model, features, labels, epochs=50)\n",
    "else:\n",
    "    print(\"Failed to fetch data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flirtingEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
